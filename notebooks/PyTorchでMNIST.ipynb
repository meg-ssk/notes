{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a380461",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "- https://rightcode.co.jp/blog/information-technology/pytorch-mnist-learning\n",
    "- https://qiita.com/TKC-tkc/items/42ff569be496621fc016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "308e148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102586ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(28*28, 1000) # MNISTは28*28*255 ≒ 20万のパターンがあるデータ。これを１０００次元にまで削減する\n",
    "        self.fc2 = torch.nn.Linear(1000, 10) # 1000成分ベクトルを10成分ベクトルにする、これが0-9の分類に対応。各成分の値は確率。\n",
    "    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x) # 活性化関数はシグモイド関数を使う\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return f.log_softmax(x, dim=1) # 出力層はsoft-max関数を使う\n",
    "\n",
    "# ニューラルネットの定義はこれだけ！\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b530e2",
   "metadata": {},
   "source": [
    "参考文献\n",
    "- https://qiita.com/fukuit/items/215ef75113d97560e599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e316b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST(batch=128, intensity=1.0):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data',\n",
    "                       train=False,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    "\n",
    "    return {'train': train_loader, 'test': test_loader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "408f4e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 2.3528034687042236\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 1.8666446208953857\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 1.3740673065185547\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 1.1048239469528198\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.835023045539856\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.6920554637908936\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.5321958065032959\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.5477780103683472\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.5142160654067993\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.4580782949924469\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.4287625551223755\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.48491013050079346\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.3890364170074463\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.5125426054000854\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.3066701591014862\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.2910521924495697\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.3599579930305481\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.30879834294319153\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.40597352385520935\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.4912177622318268\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.30219730734825134\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.30656731128692627\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.49944907426834106\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.3744713068008423\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.383829265832901\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.46777909994125366\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.35385584831237793\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.3963983952999115\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.3484734296798706\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.18613265454769135\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.2158297896385193\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.26265162229537964\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.28214022517204285\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.1616154909133911\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.21166357398033142\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.32990461587905884\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.3648592233657837\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.21204884350299835\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.24212265014648438\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.28421878814697266\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.2722732126712799\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.3714554011821747\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.19423215091228485\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.3217293918132782\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.25125524401664734\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.34485918283462524\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.3509243428707123\n",
      "Test loss (avg): 0.2671967927455902, Accuracy: 0.9199\n",
      "Training log: 2 epoch (128 / 60000 train. data). Loss: 0.24471519887447357\n",
      "Training log: 2 epoch (1408 / 60000 train. data). Loss: 0.25496944785118103\n",
      "Training log: 2 epoch (2688 / 60000 train. data). Loss: 0.31291016936302185\n",
      "Training log: 2 epoch (3968 / 60000 train. data). Loss: 0.25189799070358276\n",
      "Training log: 2 epoch (5248 / 60000 train. data). Loss: 0.1910223811864853\n",
      "Training log: 2 epoch (6528 / 60000 train. data). Loss: 0.23249590396881104\n",
      "Training log: 2 epoch (7808 / 60000 train. data). Loss: 0.19073662161827087\n",
      "Training log: 2 epoch (9088 / 60000 train. data). Loss: 0.2873879373073578\n",
      "Training log: 2 epoch (10368 / 60000 train. data). Loss: 0.26585420966148376\n",
      "Training log: 2 epoch (11648 / 60000 train. data). Loss: 0.1870081126689911\n",
      "Training log: 2 epoch (12928 / 60000 train. data). Loss: 0.32377034425735474\n",
      "Training log: 2 epoch (14208 / 60000 train. data). Loss: 0.34728455543518066\n",
      "Training log: 2 epoch (15488 / 60000 train. data). Loss: 0.29087311029434204\n",
      "Training log: 2 epoch (16768 / 60000 train. data). Loss: 0.19040407240390778\n",
      "Training log: 2 epoch (18048 / 60000 train. data). Loss: 0.290149450302124\n",
      "Training log: 2 epoch (19328 / 60000 train. data). Loss: 0.27131009101867676\n",
      "Training log: 2 epoch (20608 / 60000 train. data). Loss: 0.2712196409702301\n",
      "Training log: 2 epoch (21888 / 60000 train. data). Loss: 0.2100902646780014\n",
      "Training log: 2 epoch (23168 / 60000 train. data). Loss: 0.18256643414497375\n",
      "Training log: 2 epoch (24448 / 60000 train. data). Loss: 0.24433031678199768\n",
      "Training log: 2 epoch (25728 / 60000 train. data). Loss: 0.28702765703201294\n",
      "Training log: 2 epoch (27008 / 60000 train. data). Loss: 0.2732754349708557\n",
      "Training log: 2 epoch (28288 / 60000 train. data). Loss: 0.2491418719291687\n",
      "Training log: 2 epoch (29568 / 60000 train. data). Loss: 0.3841911554336548\n",
      "Training log: 2 epoch (30848 / 60000 train. data). Loss: 0.25813156366348267\n",
      "Training log: 2 epoch (32128 / 60000 train. data). Loss: 0.23987442255020142\n",
      "Training log: 2 epoch (33408 / 60000 train. data). Loss: 0.21104224026203156\n",
      "Training log: 2 epoch (34688 / 60000 train. data). Loss: 0.18421337008476257\n",
      "Training log: 2 epoch (35968 / 60000 train. data). Loss: 0.26268690824508667\n",
      "Training log: 2 epoch (37248 / 60000 train. data). Loss: 0.3335546851158142\n",
      "Training log: 2 epoch (38528 / 60000 train. data). Loss: 0.2243906706571579\n",
      "Training log: 2 epoch (39808 / 60000 train. data). Loss: 0.22076980769634247\n",
      "Training log: 2 epoch (41088 / 60000 train. data). Loss: 0.17165741324424744\n",
      "Training log: 2 epoch (42368 / 60000 train. data). Loss: 0.23007942736148834\n",
      "Training log: 2 epoch (43648 / 60000 train. data). Loss: 0.2499488741159439\n",
      "Training log: 2 epoch (44928 / 60000 train. data). Loss: 0.32695871591567993\n",
      "Training log: 2 epoch (46208 / 60000 train. data). Loss: 0.18700072169303894\n",
      "Training log: 2 epoch (47488 / 60000 train. data). Loss: 0.22075049579143524\n",
      "Training log: 2 epoch (48768 / 60000 train. data). Loss: 0.20747551321983337\n",
      "Training log: 2 epoch (50048 / 60000 train. data). Loss: 0.1512347310781479\n",
      "Training log: 2 epoch (51328 / 60000 train. data). Loss: 0.24145668745040894\n",
      "Training log: 2 epoch (52608 / 60000 train. data). Loss: 0.12264346331357956\n",
      "Training log: 2 epoch (53888 / 60000 train. data). Loss: 0.2997066080570221\n",
      "Training log: 2 epoch (55168 / 60000 train. data). Loss: 0.16078495979309082\n",
      "Training log: 2 epoch (56448 / 60000 train. data). Loss: 0.18045872449874878\n",
      "Training log: 2 epoch (57728 / 60000 train. data). Loss: 0.2570374608039856\n",
      "Training log: 2 epoch (59008 / 60000 train. data). Loss: 0.14877064526081085\n",
      "Test loss (avg): 0.19718850358128548, Accuracy: 0.9422\n",
      "Training log: 3 epoch (128 / 60000 train. data). Loss: 0.17191967368125916\n",
      "Training log: 3 epoch (1408 / 60000 train. data). Loss: 0.2721555531024933\n",
      "Training log: 3 epoch (2688 / 60000 train. data). Loss: 0.1538650393486023\n",
      "Training log: 3 epoch (3968 / 60000 train. data). Loss: 0.1318998634815216\n",
      "Training log: 3 epoch (5248 / 60000 train. data). Loss: 0.2377653867006302\n",
      "Training log: 3 epoch (6528 / 60000 train. data). Loss: 0.19433023035526276\n",
      "Training log: 3 epoch (7808 / 60000 train. data). Loss: 0.2592179775238037\n",
      "Training log: 3 epoch (9088 / 60000 train. data). Loss: 0.12398692220449448\n",
      "Training log: 3 epoch (10368 / 60000 train. data). Loss: 0.2511894106864929\n",
      "Training log: 3 epoch (11648 / 60000 train. data). Loss: 0.12336578965187073\n",
      "Training log: 3 epoch (12928 / 60000 train. data). Loss: 0.14264598488807678\n",
      "Training log: 3 epoch (14208 / 60000 train. data). Loss: 0.18665920197963715\n",
      "Training log: 3 epoch (15488 / 60000 train. data). Loss: 0.22911576926708221\n",
      "Training log: 3 epoch (16768 / 60000 train. data). Loss: 0.09816288948059082\n",
      "Training log: 3 epoch (18048 / 60000 train. data). Loss: 0.13655810058116913\n",
      "Training log: 3 epoch (19328 / 60000 train. data). Loss: 0.13931488990783691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 3 epoch (20608 / 60000 train. data). Loss: 0.11098121851682663\n",
      "Training log: 3 epoch (21888 / 60000 train. data). Loss: 0.21464309096336365\n",
      "Training log: 3 epoch (23168 / 60000 train. data). Loss: 0.12204588949680328\n",
      "Training log: 3 epoch (24448 / 60000 train. data). Loss: 0.10000423341989517\n",
      "Training log: 3 epoch (25728 / 60000 train. data). Loss: 0.16419266164302826\n",
      "Training log: 3 epoch (27008 / 60000 train. data). Loss: 0.21028202772140503\n",
      "Training log: 3 epoch (28288 / 60000 train. data). Loss: 0.09872440993785858\n",
      "Training log: 3 epoch (29568 / 60000 train. data). Loss: 0.1561657190322876\n",
      "Training log: 3 epoch (30848 / 60000 train. data). Loss: 0.1026986688375473\n",
      "Training log: 3 epoch (32128 / 60000 train. data). Loss: 0.19705736637115479\n",
      "Training log: 3 epoch (33408 / 60000 train. data). Loss: 0.1663469672203064\n",
      "Training log: 3 epoch (34688 / 60000 train. data). Loss: 0.34448280930519104\n",
      "Training log: 3 epoch (35968 / 60000 train. data). Loss: 0.2009119987487793\n",
      "Training log: 3 epoch (37248 / 60000 train. data). Loss: 0.13055363297462463\n",
      "Training log: 3 epoch (38528 / 60000 train. data). Loss: 0.10379938781261444\n",
      "Training log: 3 epoch (39808 / 60000 train. data). Loss: 0.1845947504043579\n",
      "Training log: 3 epoch (41088 / 60000 train. data). Loss: 0.23341871798038483\n",
      "Training log: 3 epoch (42368 / 60000 train. data). Loss: 0.16974307596683502\n",
      "Training log: 3 epoch (43648 / 60000 train. data). Loss: 0.14971286058425903\n",
      "Training log: 3 epoch (44928 / 60000 train. data). Loss: 0.08504348248243332\n",
      "Training log: 3 epoch (46208 / 60000 train. data). Loss: 0.1508311927318573\n",
      "Training log: 3 epoch (47488 / 60000 train. data). Loss: 0.13686342537403107\n",
      "Training log: 3 epoch (48768 / 60000 train. data). Loss: 0.20293913781642914\n",
      "Training log: 3 epoch (50048 / 60000 train. data). Loss: 0.16685494780540466\n",
      "Training log: 3 epoch (51328 / 60000 train. data). Loss: 0.28653812408447266\n",
      "Training log: 3 epoch (52608 / 60000 train. data). Loss: 0.1789632886648178\n",
      "Training log: 3 epoch (53888 / 60000 train. data). Loss: 0.08504641801118851\n",
      "Training log: 3 epoch (55168 / 60000 train. data). Loss: 0.2659662663936615\n",
      "Training log: 3 epoch (56448 / 60000 train. data). Loss: 0.18587495386600494\n",
      "Training log: 3 epoch (57728 / 60000 train. data). Loss: 0.1516161561012268\n",
      "Training log: 3 epoch (59008 / 60000 train. data). Loss: 0.15724115073680878\n",
      "Test loss (avg): 0.1561563427209854, Accuracy: 0.9536\n",
      "Training log: 4 epoch (128 / 60000 train. data). Loss: 0.19589488208293915\n",
      "Training log: 4 epoch (1408 / 60000 train. data). Loss: 0.21803563833236694\n",
      "Training log: 4 epoch (2688 / 60000 train. data). Loss: 0.10353278368711472\n",
      "Training log: 4 epoch (3968 / 60000 train. data). Loss: 0.1807461678981781\n",
      "Training log: 4 epoch (5248 / 60000 train. data). Loss: 0.1038811057806015\n",
      "Training log: 4 epoch (6528 / 60000 train. data). Loss: 0.17818696796894073\n",
      "Training log: 4 epoch (7808 / 60000 train. data). Loss: 0.16454029083251953\n",
      "Training log: 4 epoch (9088 / 60000 train. data). Loss: 0.1349191963672638\n",
      "Training log: 4 epoch (10368 / 60000 train. data). Loss: 0.08977510780096054\n",
      "Training log: 4 epoch (11648 / 60000 train. data). Loss: 0.1491599678993225\n",
      "Training log: 4 epoch (12928 / 60000 train. data). Loss: 0.12084228545427322\n",
      "Training log: 4 epoch (14208 / 60000 train. data). Loss: 0.13147234916687012\n",
      "Training log: 4 epoch (15488 / 60000 train. data). Loss: 0.06986384838819504\n",
      "Training log: 4 epoch (16768 / 60000 train. data). Loss: 0.140250563621521\n",
      "Training log: 4 epoch (18048 / 60000 train. data). Loss: 0.19326385855674744\n",
      "Training log: 4 epoch (19328 / 60000 train. data). Loss: 0.08144761621952057\n",
      "Training log: 4 epoch (20608 / 60000 train. data). Loss: 0.19031232595443726\n",
      "Training log: 4 epoch (21888 / 60000 train. data). Loss: 0.045581135898828506\n",
      "Training log: 4 epoch (23168 / 60000 train. data). Loss: 0.077494777739048\n",
      "Training log: 4 epoch (24448 / 60000 train. data). Loss: 0.14759944379329681\n",
      "Training log: 4 epoch (25728 / 60000 train. data). Loss: 0.15961188077926636\n",
      "Training log: 4 epoch (27008 / 60000 train. data). Loss: 0.06772079318761826\n",
      "Training log: 4 epoch (28288 / 60000 train. data). Loss: 0.1432216614484787\n",
      "Training log: 4 epoch (29568 / 60000 train. data). Loss: 0.1443472057580948\n",
      "Training log: 4 epoch (30848 / 60000 train. data). Loss: 0.09183087199926376\n",
      "Training log: 4 epoch (32128 / 60000 train. data). Loss: 0.08149705827236176\n",
      "Training log: 4 epoch (33408 / 60000 train. data). Loss: 0.08123863488435745\n",
      "Training log: 4 epoch (34688 / 60000 train. data). Loss: 0.1154218316078186\n",
      "Training log: 4 epoch (35968 / 60000 train. data). Loss: 0.18109697103500366\n",
      "Training log: 4 epoch (37248 / 60000 train. data). Loss: 0.18655148148536682\n",
      "Training log: 4 epoch (38528 / 60000 train. data). Loss: 0.05130171775817871\n",
      "Training log: 4 epoch (39808 / 60000 train. data). Loss: 0.11881402879953384\n",
      "Training log: 4 epoch (41088 / 60000 train. data). Loss: 0.13774548470973969\n",
      "Training log: 4 epoch (42368 / 60000 train. data). Loss: 0.13433359563350677\n",
      "Training log: 4 epoch (43648 / 60000 train. data). Loss: 0.06612775474786758\n",
      "Training log: 4 epoch (44928 / 60000 train. data). Loss: 0.07799025624990463\n",
      "Training log: 4 epoch (46208 / 60000 train. data). Loss: 0.04175000637769699\n",
      "Training log: 4 epoch (47488 / 60000 train. data). Loss: 0.07678314298391342\n",
      "Training log: 4 epoch (48768 / 60000 train. data). Loss: 0.11340945959091187\n",
      "Training log: 4 epoch (50048 / 60000 train. data). Loss: 0.07849095016717911\n",
      "Training log: 4 epoch (51328 / 60000 train. data). Loss: 0.12014380097389221\n",
      "Training log: 4 epoch (52608 / 60000 train. data). Loss: 0.1957404762506485\n",
      "Training log: 4 epoch (53888 / 60000 train. data). Loss: 0.11045065522193909\n",
      "Training log: 4 epoch (55168 / 60000 train. data). Loss: 0.07168832421302795\n",
      "Training log: 4 epoch (56448 / 60000 train. data). Loss: 0.09984564781188965\n",
      "Training log: 4 epoch (57728 / 60000 train. data). Loss: 0.07252643257379532\n",
      "Training log: 4 epoch (59008 / 60000 train. data). Loss: 0.23660999536514282\n",
      "Test loss (avg): 0.12714905890226363, Accuracy: 0.9615\n",
      "Training log: 5 epoch (128 / 60000 train. data). Loss: 0.06936566531658173\n",
      "Training log: 5 epoch (1408 / 60000 train. data). Loss: 0.14492525160312653\n",
      "Training log: 5 epoch (2688 / 60000 train. data). Loss: 0.06210809946060181\n",
      "Training log: 5 epoch (3968 / 60000 train. data). Loss: 0.08248323202133179\n",
      "Training log: 5 epoch (5248 / 60000 train. data). Loss: 0.13991358876228333\n",
      "Training log: 5 epoch (6528 / 60000 train. data). Loss: 0.09518302977085114\n",
      "Training log: 5 epoch (7808 / 60000 train. data). Loss: 0.10277321189641953\n",
      "Training log: 5 epoch (9088 / 60000 train. data). Loss: 0.18501617014408112\n",
      "Training log: 5 epoch (10368 / 60000 train. data). Loss: 0.10554803907871246\n",
      "Training log: 5 epoch (11648 / 60000 train. data). Loss: 0.05179530754685402\n",
      "Training log: 5 epoch (12928 / 60000 train. data). Loss: 0.09225891530513763\n",
      "Training log: 5 epoch (14208 / 60000 train. data). Loss: 0.11109402775764465\n",
      "Training log: 5 epoch (15488 / 60000 train. data). Loss: 0.1493607759475708\n",
      "Training log: 5 epoch (16768 / 60000 train. data). Loss: 0.09900766611099243\n",
      "Training log: 5 epoch (18048 / 60000 train. data). Loss: 0.12745875120162964\n",
      "Training log: 5 epoch (19328 / 60000 train. data). Loss: 0.12022390216588974\n",
      "Training log: 5 epoch (20608 / 60000 train. data). Loss: 0.04257163777947426\n",
      "Training log: 5 epoch (21888 / 60000 train. data). Loss: 0.06869696825742722\n",
      "Training log: 5 epoch (23168 / 60000 train. data). Loss: 0.10336019098758698\n",
      "Training log: 5 epoch (24448 / 60000 train. data). Loss: 0.1162915900349617\n",
      "Training log: 5 epoch (25728 / 60000 train. data). Loss: 0.07474855333566666\n",
      "Training log: 5 epoch (27008 / 60000 train. data). Loss: 0.07947203516960144\n",
      "Training log: 5 epoch (28288 / 60000 train. data). Loss: 0.13339005410671234\n",
      "Training log: 5 epoch (29568 / 60000 train. data). Loss: 0.05852165073156357\n",
      "Training log: 5 epoch (30848 / 60000 train. data). Loss: 0.08241765201091766\n",
      "Training log: 5 epoch (32128 / 60000 train. data). Loss: 0.09328507632017136\n",
      "Training log: 5 epoch (33408 / 60000 train. data). Loss: 0.06212245672941208\n",
      "Training log: 5 epoch (34688 / 60000 train. data). Loss: 0.08191408216953278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 5 epoch (35968 / 60000 train. data). Loss: 0.08487856388092041\n",
      "Training log: 5 epoch (37248 / 60000 train. data). Loss: 0.10410808771848679\n",
      "Training log: 5 epoch (38528 / 60000 train. data). Loss: 0.1325554996728897\n",
      "Training log: 5 epoch (39808 / 60000 train. data). Loss: 0.15793463587760925\n",
      "Training log: 5 epoch (41088 / 60000 train. data). Loss: 0.13342709839344025\n",
      "Training log: 5 epoch (42368 / 60000 train. data). Loss: 0.07418759167194366\n",
      "Training log: 5 epoch (43648 / 60000 train. data). Loss: 0.07851682603359222\n",
      "Training log: 5 epoch (44928 / 60000 train. data). Loss: 0.0962606742978096\n",
      "Training log: 5 epoch (46208 / 60000 train. data). Loss: 0.20388343930244446\n",
      "Training log: 5 epoch (47488 / 60000 train. data). Loss: 0.09209572523832321\n",
      "Training log: 5 epoch (48768 / 60000 train. data). Loss: 0.05876302719116211\n",
      "Training log: 5 epoch (50048 / 60000 train. data). Loss: 0.07931571453809738\n",
      "Training log: 5 epoch (51328 / 60000 train. data). Loss: 0.1314748078584671\n",
      "Training log: 5 epoch (52608 / 60000 train. data). Loss: 0.1196548193693161\n",
      "Training log: 5 epoch (53888 / 60000 train. data). Loss: 0.07673889398574829\n",
      "Training log: 5 epoch (55168 / 60000 train. data). Loss: 0.039889004081487656\n",
      "Training log: 5 epoch (56448 / 60000 train. data). Loss: 0.09586140513420105\n",
      "Training log: 5 epoch (57728 / 60000 train. data). Loss: 0.17617443203926086\n",
      "Training log: 5 epoch (59008 / 60000 train. data). Loss: 0.11028965562582016\n",
      "Test loss (avg): 0.10901017000675202, Accuracy: 0.966\n",
      "Training log: 6 epoch (128 / 60000 train. data). Loss: 0.06619015336036682\n",
      "Training log: 6 epoch (1408 / 60000 train. data). Loss: 0.026609471067786217\n",
      "Training log: 6 epoch (2688 / 60000 train. data). Loss: 0.17580784857273102\n",
      "Training log: 6 epoch (3968 / 60000 train. data). Loss: 0.023504070937633514\n",
      "Training log: 6 epoch (5248 / 60000 train. data). Loss: 0.06076402589678764\n",
      "Training log: 6 epoch (6528 / 60000 train. data). Loss: 0.07892970740795135\n",
      "Training log: 6 epoch (7808 / 60000 train. data). Loss: 0.10118945688009262\n",
      "Training log: 6 epoch (9088 / 60000 train. data). Loss: 0.13374397158622742\n",
      "Training log: 6 epoch (10368 / 60000 train. data). Loss: 0.049631260335445404\n",
      "Training log: 6 epoch (11648 / 60000 train. data). Loss: 0.12421903759241104\n",
      "Training log: 6 epoch (12928 / 60000 train. data). Loss: 0.05064769089221954\n",
      "Training log: 6 epoch (14208 / 60000 train. data). Loss: 0.11048473417758942\n",
      "Training log: 6 epoch (15488 / 60000 train. data). Loss: 0.08551172912120819\n",
      "Training log: 6 epoch (16768 / 60000 train. data). Loss: 0.14427264034748077\n",
      "Training log: 6 epoch (18048 / 60000 train. data). Loss: 0.10239046812057495\n",
      "Training log: 6 epoch (19328 / 60000 train. data). Loss: 0.12441107630729675\n",
      "Training log: 6 epoch (20608 / 60000 train. data). Loss: 0.0236015897244215\n",
      "Training log: 6 epoch (21888 / 60000 train. data). Loss: 0.09987807273864746\n",
      "Training log: 6 epoch (23168 / 60000 train. data). Loss: 0.15372319519519806\n",
      "Training log: 6 epoch (24448 / 60000 train. data). Loss: 0.09818698465824127\n",
      "Training log: 6 epoch (25728 / 60000 train. data). Loss: 0.1446807086467743\n",
      "Training log: 6 epoch (27008 / 60000 train. data). Loss: 0.0435672402381897\n",
      "Training log: 6 epoch (28288 / 60000 train. data). Loss: 0.056927766650915146\n",
      "Training log: 6 epoch (29568 / 60000 train. data). Loss: 0.05256662890315056\n",
      "Training log: 6 epoch (30848 / 60000 train. data). Loss: 0.08220575749874115\n",
      "Training log: 6 epoch (32128 / 60000 train. data). Loss: 0.13218891620635986\n",
      "Training log: 6 epoch (33408 / 60000 train. data). Loss: 0.056977152824401855\n",
      "Training log: 6 epoch (34688 / 60000 train. data). Loss: 0.0648629292845726\n",
      "Training log: 6 epoch (35968 / 60000 train. data). Loss: 0.06031956896185875\n",
      "Training log: 6 epoch (37248 / 60000 train. data). Loss: 0.05821135640144348\n",
      "Training log: 6 epoch (38528 / 60000 train. data). Loss: 0.05600862205028534\n",
      "Training log: 6 epoch (39808 / 60000 train. data). Loss: 0.08614473044872284\n",
      "Training log: 6 epoch (41088 / 60000 train. data). Loss: 0.07052839547395706\n",
      "Training log: 6 epoch (42368 / 60000 train. data). Loss: 0.02667774073779583\n",
      "Training log: 6 epoch (43648 / 60000 train. data). Loss: 0.10746466368436813\n",
      "Training log: 6 epoch (44928 / 60000 train. data). Loss: 0.07300560176372528\n",
      "Training log: 6 epoch (46208 / 60000 train. data). Loss: 0.07190245389938354\n",
      "Training log: 6 epoch (47488 / 60000 train. data). Loss: 0.056047841906547546\n",
      "Training log: 6 epoch (48768 / 60000 train. data). Loss: 0.0613328218460083\n",
      "Training log: 6 epoch (50048 / 60000 train. data). Loss: 0.06492677330970764\n",
      "Training log: 6 epoch (51328 / 60000 train. data). Loss: 0.19726726412773132\n",
      "Training log: 6 epoch (52608 / 60000 train. data). Loss: 0.11832872778177261\n",
      "Training log: 6 epoch (53888 / 60000 train. data). Loss: 0.10216987133026123\n",
      "Training log: 6 epoch (55168 / 60000 train. data). Loss: 0.08018192648887634\n",
      "Training log: 6 epoch (56448 / 60000 train. data). Loss: 0.053899165242910385\n",
      "Training log: 6 epoch (57728 / 60000 train. data). Loss: 0.10782142728567123\n",
      "Training log: 6 epoch (59008 / 60000 train. data). Loss: 0.07415942847728729\n",
      "Test loss (avg): 0.09553101639971137, Accuracy: 0.9708\n",
      "Training log: 7 epoch (128 / 60000 train. data). Loss: 0.06976213306188583\n",
      "Training log: 7 epoch (1408 / 60000 train. data). Loss: 0.10499832779169083\n",
      "Training log: 7 epoch (2688 / 60000 train. data). Loss: 0.047713134437799454\n",
      "Training log: 7 epoch (3968 / 60000 train. data). Loss: 0.07814215123653412\n",
      "Training log: 7 epoch (5248 / 60000 train. data). Loss: 0.08013106137514114\n",
      "Training log: 7 epoch (6528 / 60000 train. data). Loss: 0.02510032244026661\n",
      "Training log: 7 epoch (7808 / 60000 train. data). Loss: 0.08066844940185547\n",
      "Training log: 7 epoch (9088 / 60000 train. data). Loss: 0.05351140350103378\n",
      "Training log: 7 epoch (10368 / 60000 train. data). Loss: 0.05343117192387581\n",
      "Training log: 7 epoch (11648 / 60000 train. data). Loss: 0.06830315291881561\n",
      "Training log: 7 epoch (12928 / 60000 train. data). Loss: 0.07145875692367554\n",
      "Training log: 7 epoch (14208 / 60000 train. data). Loss: 0.07542285323143005\n",
      "Training log: 7 epoch (15488 / 60000 train. data). Loss: 0.05908650904893875\n",
      "Training log: 7 epoch (16768 / 60000 train. data). Loss: 0.09139198064804077\n",
      "Training log: 7 epoch (18048 / 60000 train. data). Loss: 0.08763903379440308\n",
      "Training log: 7 epoch (19328 / 60000 train. data). Loss: 0.05591633915901184\n",
      "Training log: 7 epoch (20608 / 60000 train. data). Loss: 0.05929506570100784\n",
      "Training log: 7 epoch (21888 / 60000 train. data). Loss: 0.08868714421987534\n",
      "Training log: 7 epoch (23168 / 60000 train. data). Loss: 0.04905211180448532\n",
      "Training log: 7 epoch (24448 / 60000 train. data). Loss: 0.09916768968105316\n",
      "Training log: 7 epoch (25728 / 60000 train. data). Loss: 0.06585216522216797\n",
      "Training log: 7 epoch (27008 / 60000 train. data). Loss: 0.07787429541349411\n",
      "Training log: 7 epoch (28288 / 60000 train. data). Loss: 0.07845395058393478\n",
      "Training log: 7 epoch (29568 / 60000 train. data). Loss: 0.02298027276992798\n",
      "Training log: 7 epoch (30848 / 60000 train. data). Loss: 0.0997869223356247\n",
      "Training log: 7 epoch (32128 / 60000 train. data). Loss: 0.04652857035398483\n",
      "Training log: 7 epoch (33408 / 60000 train. data). Loss: 0.060280971229076385\n",
      "Training log: 7 epoch (34688 / 60000 train. data). Loss: 0.0737227126955986\n",
      "Training log: 7 epoch (35968 / 60000 train. data). Loss: 0.13596579432487488\n",
      "Training log: 7 epoch (37248 / 60000 train. data). Loss: 0.0919426903128624\n",
      "Training log: 7 epoch (38528 / 60000 train. data). Loss: 0.043956682085990906\n",
      "Training log: 7 epoch (39808 / 60000 train. data). Loss: 0.041978005319833755\n",
      "Training log: 7 epoch (41088 / 60000 train. data). Loss: 0.05854666978120804\n",
      "Training log: 7 epoch (42368 / 60000 train. data). Loss: 0.05844404175877571\n",
      "Training log: 7 epoch (43648 / 60000 train. data). Loss: 0.11953283846378326\n",
      "Training log: 7 epoch (44928 / 60000 train. data). Loss: 0.03288960084319115\n",
      "Training log: 7 epoch (46208 / 60000 train. data). Loss: 0.05066591128706932\n",
      "Training log: 7 epoch (47488 / 60000 train. data). Loss: 0.05579451099038124\n",
      "Training log: 7 epoch (48768 / 60000 train. data). Loss: 0.08949700742959976\n",
      "Training log: 7 epoch (50048 / 60000 train. data). Loss: 0.03443238511681557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 7 epoch (51328 / 60000 train. data). Loss: 0.11067335307598114\n",
      "Training log: 7 epoch (52608 / 60000 train. data). Loss: 0.027964910492300987\n",
      "Training log: 7 epoch (53888 / 60000 train. data). Loss: 0.06237402185797691\n",
      "Training log: 7 epoch (55168 / 60000 train. data). Loss: 0.1303621530532837\n",
      "Training log: 7 epoch (56448 / 60000 train. data). Loss: 0.09952236711978912\n",
      "Training log: 7 epoch (57728 / 60000 train. data). Loss: 0.1424628049135208\n",
      "Training log: 7 epoch (59008 / 60000 train. data). Loss: 0.046975139528512955\n",
      "Test loss (avg): 0.09974847429990769, Accuracy: 0.9702\n",
      "Training log: 8 epoch (128 / 60000 train. data). Loss: 0.10895831882953644\n",
      "Training log: 8 epoch (1408 / 60000 train. data). Loss: 0.06590691208839417\n",
      "Training log: 8 epoch (2688 / 60000 train. data). Loss: 0.03491322323679924\n",
      "Training log: 8 epoch (3968 / 60000 train. data). Loss: 0.08711224794387817\n",
      "Training log: 8 epoch (5248 / 60000 train. data). Loss: 0.02525004930794239\n",
      "Training log: 8 epoch (6528 / 60000 train. data). Loss: 0.10082057118415833\n",
      "Training log: 8 epoch (7808 / 60000 train. data). Loss: 0.03312266618013382\n",
      "Training log: 8 epoch (9088 / 60000 train. data). Loss: 0.06769327074289322\n",
      "Training log: 8 epoch (10368 / 60000 train. data). Loss: 0.022977685555815697\n",
      "Training log: 8 epoch (11648 / 60000 train. data). Loss: 0.022257110103964806\n",
      "Training log: 8 epoch (12928 / 60000 train. data). Loss: 0.05541902035474777\n",
      "Training log: 8 epoch (14208 / 60000 train. data). Loss: 0.036413490772247314\n",
      "Training log: 8 epoch (15488 / 60000 train. data). Loss: 0.05586725100874901\n",
      "Training log: 8 epoch (16768 / 60000 train. data). Loss: 0.08227483183145523\n",
      "Training log: 8 epoch (18048 / 60000 train. data). Loss: 0.06199513375759125\n",
      "Training log: 8 epoch (19328 / 60000 train. data). Loss: 0.043379101902246475\n",
      "Training log: 8 epoch (20608 / 60000 train. data). Loss: 0.07389180362224579\n",
      "Training log: 8 epoch (21888 / 60000 train. data). Loss: 0.08374800533056259\n",
      "Training log: 8 epoch (23168 / 60000 train. data). Loss: 0.08847061544656754\n",
      "Training log: 8 epoch (24448 / 60000 train. data). Loss: 0.05287790298461914\n",
      "Training log: 8 epoch (25728 / 60000 train. data). Loss: 0.021718857809901237\n",
      "Training log: 8 epoch (27008 / 60000 train. data). Loss: 0.01719423569738865\n",
      "Training log: 8 epoch (28288 / 60000 train. data). Loss: 0.023163093253970146\n",
      "Training log: 8 epoch (29568 / 60000 train. data). Loss: 0.05127809941768646\n",
      "Training log: 8 epoch (30848 / 60000 train. data). Loss: 0.08626210689544678\n",
      "Training log: 8 epoch (32128 / 60000 train. data). Loss: 0.042254362255334854\n",
      "Training log: 8 epoch (33408 / 60000 train. data). Loss: 0.08276517689228058\n",
      "Training log: 8 epoch (34688 / 60000 train. data). Loss: 0.048125170171260834\n",
      "Training log: 8 epoch (35968 / 60000 train. data). Loss: 0.0681595504283905\n",
      "Training log: 8 epoch (37248 / 60000 train. data). Loss: 0.08975261449813843\n",
      "Training log: 8 epoch (38528 / 60000 train. data). Loss: 0.04261528328061104\n",
      "Training log: 8 epoch (39808 / 60000 train. data). Loss: 0.03259649500250816\n",
      "Training log: 8 epoch (41088 / 60000 train. data). Loss: 0.03895692899823189\n",
      "Training log: 8 epoch (42368 / 60000 train. data). Loss: 0.03376093879342079\n",
      "Training log: 8 epoch (43648 / 60000 train. data). Loss: 0.03902211785316467\n",
      "Training log: 8 epoch (44928 / 60000 train. data). Loss: 0.07303263247013092\n",
      "Training log: 8 epoch (46208 / 60000 train. data). Loss: 0.010864469222724438\n",
      "Training log: 8 epoch (47488 / 60000 train. data). Loss: 0.07406264543533325\n",
      "Training log: 8 epoch (48768 / 60000 train. data). Loss: 0.06913508474826813\n",
      "Training log: 8 epoch (50048 / 60000 train. data). Loss: 0.04275430738925934\n",
      "Training log: 8 epoch (51328 / 60000 train. data). Loss: 0.20134282112121582\n",
      "Training log: 8 epoch (52608 / 60000 train. data). Loss: 0.04751524329185486\n",
      "Training log: 8 epoch (53888 / 60000 train. data). Loss: 0.08262431621551514\n",
      "Training log: 8 epoch (55168 / 60000 train. data). Loss: 0.03687018156051636\n",
      "Training log: 8 epoch (56448 / 60000 train. data). Loss: 0.09152967482805252\n",
      "Training log: 8 epoch (57728 / 60000 train. data). Loss: 0.07843662798404694\n",
      "Training log: 8 epoch (59008 / 60000 train. data). Loss: 0.02041105180978775\n",
      "Test loss (avg): 0.07834099078178405, Accuracy: 0.9766\n",
      "Training log: 9 epoch (128 / 60000 train. data). Loss: 0.03492086008191109\n",
      "Training log: 9 epoch (1408 / 60000 train. data). Loss: 0.014321624301373959\n",
      "Training log: 9 epoch (2688 / 60000 train. data). Loss: 0.039107318967580795\n",
      "Training log: 9 epoch (3968 / 60000 train. data). Loss: 0.05236751213669777\n",
      "Training log: 9 epoch (5248 / 60000 train. data). Loss: 0.08305411785840988\n",
      "Training log: 9 epoch (6528 / 60000 train. data). Loss: 0.039979416877031326\n",
      "Training log: 9 epoch (7808 / 60000 train. data). Loss: 0.02794700115919113\n",
      "Training log: 9 epoch (9088 / 60000 train. data). Loss: 0.04018491506576538\n",
      "Training log: 9 epoch (10368 / 60000 train. data). Loss: 0.08820884674787521\n",
      "Training log: 9 epoch (11648 / 60000 train. data). Loss: 0.10241179168224335\n",
      "Training log: 9 epoch (12928 / 60000 train. data). Loss: 0.02814408577978611\n",
      "Training log: 9 epoch (14208 / 60000 train. data). Loss: 0.04862619936466217\n",
      "Training log: 9 epoch (15488 / 60000 train. data). Loss: 0.08669137209653854\n",
      "Training log: 9 epoch (16768 / 60000 train. data). Loss: 0.04073163866996765\n",
      "Training log: 9 epoch (18048 / 60000 train. data). Loss: 0.06017744913697243\n",
      "Training log: 9 epoch (19328 / 60000 train. data). Loss: 0.07597169280052185\n",
      "Training log: 9 epoch (20608 / 60000 train. data). Loss: 0.01598908193409443\n",
      "Training log: 9 epoch (21888 / 60000 train. data). Loss: 0.08007445931434631\n",
      "Training log: 9 epoch (23168 / 60000 train. data). Loss: 0.06672079861164093\n",
      "Training log: 9 epoch (24448 / 60000 train. data). Loss: 0.02498193457722664\n",
      "Training log: 9 epoch (25728 / 60000 train. data). Loss: 0.02063816785812378\n",
      "Training log: 9 epoch (27008 / 60000 train. data). Loss: 0.10466822236776352\n",
      "Training log: 9 epoch (28288 / 60000 train. data). Loss: 0.07198868691921234\n",
      "Training log: 9 epoch (29568 / 60000 train. data). Loss: 0.042104873806238174\n",
      "Training log: 9 epoch (30848 / 60000 train. data). Loss: 0.0439087375998497\n",
      "Training log: 9 epoch (32128 / 60000 train. data). Loss: 0.031209424138069153\n",
      "Training log: 9 epoch (33408 / 60000 train. data). Loss: 0.05908291041851044\n",
      "Training log: 9 epoch (34688 / 60000 train. data). Loss: 0.03151681274175644\n",
      "Training log: 9 epoch (35968 / 60000 train. data). Loss: 0.035012539476156235\n",
      "Training log: 9 epoch (37248 / 60000 train. data). Loss: 0.032793499529361725\n",
      "Training log: 9 epoch (38528 / 60000 train. data). Loss: 0.056601159274578094\n",
      "Training log: 9 epoch (39808 / 60000 train. data). Loss: 0.08475351333618164\n",
      "Training log: 9 epoch (41088 / 60000 train. data). Loss: 0.021771928295493126\n",
      "Training log: 9 epoch (42368 / 60000 train. data). Loss: 0.059702467173337936\n",
      "Training log: 9 epoch (43648 / 60000 train. data). Loss: 0.05953066796064377\n",
      "Training log: 9 epoch (44928 / 60000 train. data). Loss: 0.08569300174713135\n",
      "Training log: 9 epoch (46208 / 60000 train. data). Loss: 0.04014488309621811\n",
      "Training log: 9 epoch (47488 / 60000 train. data). Loss: 0.029049599543213844\n",
      "Training log: 9 epoch (48768 / 60000 train. data). Loss: 0.09392863512039185\n",
      "Training log: 9 epoch (50048 / 60000 train. data). Loss: 0.056763745844364166\n",
      "Training log: 9 epoch (51328 / 60000 train. data). Loss: 0.03581174463033676\n",
      "Training log: 9 epoch (52608 / 60000 train. data). Loss: 0.04754766821861267\n",
      "Training log: 9 epoch (53888 / 60000 train. data). Loss: 0.02716607227921486\n",
      "Training log: 9 epoch (55168 / 60000 train. data). Loss: 0.031056800857186317\n",
      "Training log: 9 epoch (56448 / 60000 train. data). Loss: 0.02659153752028942\n",
      "Training log: 9 epoch (57728 / 60000 train. data). Loss: 0.10767518728971481\n",
      "Training log: 9 epoch (59008 / 60000 train. data). Loss: 0.032239191234111786\n",
      "Test loss (avg): 0.07949951520860195, Accuracy: 0.9757\n",
      "Training log: 10 epoch (128 / 60000 train. data). Loss: 0.033162690699100494\n",
      "Training log: 10 epoch (1408 / 60000 train. data). Loss: 0.02583272196352482\n",
      "Training log: 10 epoch (2688 / 60000 train. data). Loss: 0.06486683338880539\n",
      "Training log: 10 epoch (3968 / 60000 train. data). Loss: 0.09781279414892197\n",
      "Training log: 10 epoch (5248 / 60000 train. data). Loss: 0.060599807649850845\n",
      "Training log: 10 epoch (6528 / 60000 train. data). Loss: 0.0620105154812336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 10 epoch (7808 / 60000 train. data). Loss: 0.021628227084875107\n",
      "Training log: 10 epoch (9088 / 60000 train. data). Loss: 0.13621786236763\n",
      "Training log: 10 epoch (10368 / 60000 train. data). Loss: 0.04509781673550606\n",
      "Training log: 10 epoch (11648 / 60000 train. data). Loss: 0.05147406458854675\n",
      "Training log: 10 epoch (12928 / 60000 train. data). Loss: 0.06953559070825577\n",
      "Training log: 10 epoch (14208 / 60000 train. data). Loss: 0.014737465418875217\n",
      "Training log: 10 epoch (15488 / 60000 train. data). Loss: 0.09268917888402939\n",
      "Training log: 10 epoch (16768 / 60000 train. data). Loss: 0.019133800640702248\n",
      "Training log: 10 epoch (18048 / 60000 train. data). Loss: 0.0484098382294178\n",
      "Training log: 10 epoch (19328 / 60000 train. data). Loss: 0.08893236517906189\n",
      "Training log: 10 epoch (20608 / 60000 train. data). Loss: 0.02538367733359337\n",
      "Training log: 10 epoch (21888 / 60000 train. data). Loss: 0.03560527041554451\n",
      "Training log: 10 epoch (23168 / 60000 train. data). Loss: 0.06844136118888855\n",
      "Training log: 10 epoch (24448 / 60000 train. data). Loss: 0.06485703587532043\n",
      "Training log: 10 epoch (25728 / 60000 train. data). Loss: 0.04848445579409599\n",
      "Training log: 10 epoch (27008 / 60000 train. data). Loss: 0.05152183398604393\n",
      "Training log: 10 epoch (28288 / 60000 train. data). Loss: 0.03793688863515854\n",
      "Training log: 10 epoch (29568 / 60000 train. data). Loss: 0.02570316195487976\n",
      "Training log: 10 epoch (30848 / 60000 train. data). Loss: 0.0373384952545166\n",
      "Training log: 10 epoch (32128 / 60000 train. data). Loss: 0.04516644775867462\n",
      "Training log: 10 epoch (33408 / 60000 train. data). Loss: 0.01391259953379631\n",
      "Training log: 10 epoch (34688 / 60000 train. data). Loss: 0.02834084816277027\n",
      "Training log: 10 epoch (35968 / 60000 train. data). Loss: 0.03481863811612129\n",
      "Training log: 10 epoch (37248 / 60000 train. data). Loss: 0.05402040109038353\n",
      "Training log: 10 epoch (38528 / 60000 train. data). Loss: 0.07939325273036957\n",
      "Training log: 10 epoch (39808 / 60000 train. data). Loss: 0.041796017438173294\n",
      "Training log: 10 epoch (41088 / 60000 train. data). Loss: 0.04762357473373413\n",
      "Training log: 10 epoch (42368 / 60000 train. data). Loss: 0.05309871584177017\n",
      "Training log: 10 epoch (43648 / 60000 train. data). Loss: 0.023534655570983887\n",
      "Training log: 10 epoch (44928 / 60000 train. data). Loss: 0.014226350001990795\n",
      "Training log: 10 epoch (46208 / 60000 train. data). Loss: 0.019968967884778976\n",
      "Training log: 10 epoch (47488 / 60000 train. data). Loss: 0.024779846891760826\n",
      "Training log: 10 epoch (48768 / 60000 train. data). Loss: 0.02608739584684372\n",
      "Training log: 10 epoch (50048 / 60000 train. data). Loss: 0.018119318410754204\n",
      "Training log: 10 epoch (51328 / 60000 train. data). Loss: 0.015258176252245903\n",
      "Training log: 10 epoch (52608 / 60000 train. data). Loss: 0.03255071863532066\n",
      "Training log: 10 epoch (53888 / 60000 train. data). Loss: 0.04483723267912865\n",
      "Training log: 10 epoch (55168 / 60000 train. data). Loss: 0.04479130730032921\n",
      "Training log: 10 epoch (56448 / 60000 train. data). Loss: 0.02785899117588997\n",
      "Training log: 10 epoch (57728 / 60000 train. data). Loss: 0.05144035443663597\n",
      "Training log: 10 epoch (59008 / 60000 train. data). Loss: 0.02818642370402813\n",
      "Test loss (avg): 0.06837991048395634, Accuracy: 0.9792\n",
      "Training log: 11 epoch (128 / 60000 train. data). Loss: 0.03488702327013016\n",
      "Training log: 11 epoch (1408 / 60000 train. data). Loss: 0.045640163123607635\n",
      "Training log: 11 epoch (2688 / 60000 train. data). Loss: 0.023964934051036835\n",
      "Training log: 11 epoch (3968 / 60000 train. data). Loss: 0.04356847703456879\n",
      "Training log: 11 epoch (5248 / 60000 train. data). Loss: 0.011445099487900734\n",
      "Training log: 11 epoch (6528 / 60000 train. data). Loss: 0.027107203379273415\n",
      "Training log: 11 epoch (7808 / 60000 train. data). Loss: 0.02026117593050003\n",
      "Training log: 11 epoch (9088 / 60000 train. data). Loss: 0.014948345720767975\n",
      "Training log: 11 epoch (10368 / 60000 train. data). Loss: 0.02861815318465233\n",
      "Training log: 11 epoch (11648 / 60000 train. data). Loss: 0.039023302495479584\n",
      "Training log: 11 epoch (12928 / 60000 train. data). Loss: 0.018187042325735092\n",
      "Training log: 11 epoch (14208 / 60000 train. data). Loss: 0.045035943388938904\n",
      "Training log: 11 epoch (15488 / 60000 train. data). Loss: 0.06368234008550644\n",
      "Training log: 11 epoch (16768 / 60000 train. data). Loss: 0.03271462768316269\n",
      "Training log: 11 epoch (18048 / 60000 train. data). Loss: 0.04229627177119255\n",
      "Training log: 11 epoch (19328 / 60000 train. data). Loss: 0.019325364381074905\n",
      "Training log: 11 epoch (20608 / 60000 train. data). Loss: 0.09071705490350723\n",
      "Training log: 11 epoch (21888 / 60000 train. data). Loss: 0.01527971588075161\n",
      "Training log: 11 epoch (23168 / 60000 train. data). Loss: 0.016770459711551666\n",
      "Training log: 11 epoch (24448 / 60000 train. data). Loss: 0.0169514250010252\n",
      "Training log: 11 epoch (25728 / 60000 train. data). Loss: 0.037695690989494324\n",
      "Training log: 11 epoch (27008 / 60000 train. data). Loss: 0.012427463196218014\n",
      "Training log: 11 epoch (28288 / 60000 train. data). Loss: 0.017242304980754852\n",
      "Training log: 11 epoch (29568 / 60000 train. data). Loss: 0.01668059639632702\n",
      "Training log: 11 epoch (30848 / 60000 train. data). Loss: 0.00797274149954319\n",
      "Training log: 11 epoch (32128 / 60000 train. data). Loss: 0.02345014177262783\n",
      "Training log: 11 epoch (33408 / 60000 train. data). Loss: 0.06215449050068855\n",
      "Training log: 11 epoch (34688 / 60000 train. data). Loss: 0.05188288912177086\n",
      "Training log: 11 epoch (35968 / 60000 train. data). Loss: 0.07641492038965225\n",
      "Training log: 11 epoch (37248 / 60000 train. data). Loss: 0.02205287106335163\n",
      "Training log: 11 epoch (38528 / 60000 train. data). Loss: 0.02368730492889881\n",
      "Training log: 11 epoch (39808 / 60000 train. data). Loss: 0.025164136663079262\n",
      "Training log: 11 epoch (41088 / 60000 train. data). Loss: 0.00400699395686388\n",
      "Training log: 11 epoch (42368 / 60000 train. data). Loss: 0.02387530542910099\n",
      "Training log: 11 epoch (43648 / 60000 train. data). Loss: 0.12125110626220703\n",
      "Training log: 11 epoch (44928 / 60000 train. data). Loss: 0.03722119331359863\n",
      "Training log: 11 epoch (46208 / 60000 train. data). Loss: 0.023214947432279587\n",
      "Training log: 11 epoch (47488 / 60000 train. data). Loss: 0.01985306292772293\n",
      "Training log: 11 epoch (48768 / 60000 train. data). Loss: 0.028200456872582436\n",
      "Training log: 11 epoch (50048 / 60000 train. data). Loss: 0.08362200856208801\n",
      "Training log: 11 epoch (51328 / 60000 train. data). Loss: 0.024143652990460396\n",
      "Training log: 11 epoch (52608 / 60000 train. data). Loss: 0.037946950644254684\n",
      "Training log: 11 epoch (53888 / 60000 train. data). Loss: 0.07080763578414917\n",
      "Training log: 11 epoch (55168 / 60000 train. data). Loss: 0.01112504955381155\n",
      "Training log: 11 epoch (56448 / 60000 train. data). Loss: 0.047184817492961884\n",
      "Training log: 11 epoch (57728 / 60000 train. data). Loss: 0.012624289840459824\n",
      "Training log: 11 epoch (59008 / 60000 train. data). Loss: 0.09678815305233002\n",
      "Test loss (avg): 0.06632887320816516, Accuracy: 0.9799\n",
      "Training log: 12 epoch (128 / 60000 train. data). Loss: 0.025666672736406326\n",
      "Training log: 12 epoch (1408 / 60000 train. data). Loss: 0.02924228645861149\n",
      "Training log: 12 epoch (2688 / 60000 train. data). Loss: 0.06101486459374428\n",
      "Training log: 12 epoch (3968 / 60000 train. data). Loss: 0.022528255358338356\n",
      "Training log: 12 epoch (5248 / 60000 train. data). Loss: 0.009239044040441513\n",
      "Training log: 12 epoch (6528 / 60000 train. data). Loss: 0.042698778212070465\n",
      "Training log: 12 epoch (7808 / 60000 train. data). Loss: 0.01207925658673048\n",
      "Training log: 12 epoch (9088 / 60000 train. data). Loss: 0.03049643151462078\n",
      "Training log: 12 epoch (10368 / 60000 train. data). Loss: 0.030434230342507362\n",
      "Training log: 12 epoch (11648 / 60000 train. data). Loss: 0.048423416912555695\n",
      "Training log: 12 epoch (12928 / 60000 train. data). Loss: 0.02227717638015747\n",
      "Training log: 12 epoch (14208 / 60000 train. data). Loss: 0.036098312586545944\n",
      "Training log: 12 epoch (15488 / 60000 train. data). Loss: 0.04324604570865631\n",
      "Training log: 12 epoch (16768 / 60000 train. data). Loss: 0.02515423111617565\n",
      "Training log: 12 epoch (18048 / 60000 train. data). Loss: 0.030083604156970978\n",
      "Training log: 12 epoch (19328 / 60000 train. data). Loss: 0.026898954063653946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 12 epoch (20608 / 60000 train. data). Loss: 0.020374903455376625\n",
      "Training log: 12 epoch (21888 / 60000 train. data). Loss: 0.020116746425628662\n",
      "Training log: 12 epoch (23168 / 60000 train. data). Loss: 0.08095579594373703\n",
      "Training log: 12 epoch (24448 / 60000 train. data). Loss: 0.02993413247168064\n",
      "Training log: 12 epoch (25728 / 60000 train. data). Loss: 0.020378196612000465\n",
      "Training log: 12 epoch (27008 / 60000 train. data). Loss: 0.02307731658220291\n",
      "Training log: 12 epoch (28288 / 60000 train. data). Loss: 0.12506213784217834\n",
      "Training log: 12 epoch (29568 / 60000 train. data). Loss: 0.038130830973386765\n",
      "Training log: 12 epoch (30848 / 60000 train. data). Loss: 0.07592593878507614\n",
      "Training log: 12 epoch (32128 / 60000 train. data). Loss: 0.021343737840652466\n",
      "Training log: 12 epoch (33408 / 60000 train. data). Loss: 0.01736573502421379\n",
      "Training log: 12 epoch (34688 / 60000 train. data). Loss: 0.01828763447701931\n",
      "Training log: 12 epoch (35968 / 60000 train. data). Loss: 0.041997674852609634\n",
      "Training log: 12 epoch (37248 / 60000 train. data). Loss: 0.04897983372211456\n",
      "Training log: 12 epoch (38528 / 60000 train. data). Loss: 0.03420453891158104\n",
      "Training log: 12 epoch (39808 / 60000 train. data). Loss: 0.03128046914935112\n",
      "Training log: 12 epoch (41088 / 60000 train. data). Loss: 0.05172955244779587\n",
      "Training log: 12 epoch (42368 / 60000 train. data). Loss: 0.014837547205388546\n",
      "Training log: 12 epoch (43648 / 60000 train. data). Loss: 0.03698614984750748\n",
      "Training log: 12 epoch (44928 / 60000 train. data). Loss: 0.02376280352473259\n",
      "Training log: 12 epoch (46208 / 60000 train. data). Loss: 0.016186820343136787\n",
      "Training log: 12 epoch (47488 / 60000 train. data). Loss: 0.03346937894821167\n",
      "Training log: 12 epoch (48768 / 60000 train. data). Loss: 0.01580057293176651\n",
      "Training log: 12 epoch (50048 / 60000 train. data). Loss: 0.021273162215948105\n",
      "Training log: 12 epoch (51328 / 60000 train. data). Loss: 0.007251605857163668\n",
      "Training log: 12 epoch (52608 / 60000 train. data). Loss: 0.04438816010951996\n",
      "Training log: 12 epoch (53888 / 60000 train. data). Loss: 0.050588276237249374\n",
      "Training log: 12 epoch (55168 / 60000 train. data). Loss: 0.03776169940829277\n",
      "Training log: 12 epoch (56448 / 60000 train. data). Loss: 0.007865636609494686\n",
      "Training log: 12 epoch (57728 / 60000 train. data). Loss: 0.005828023888170719\n",
      "Training log: 12 epoch (59008 / 60000 train. data). Loss: 0.024680765345692635\n",
      "Test loss (avg): 0.0643255497187376, Accuracy: 0.9807\n",
      "Training log: 13 epoch (128 / 60000 train. data). Loss: 0.020490193739533424\n",
      "Training log: 13 epoch (1408 / 60000 train. data). Loss: 0.010756740346550941\n",
      "Training log: 13 epoch (2688 / 60000 train. data). Loss: 0.015965500846505165\n",
      "Training log: 13 epoch (3968 / 60000 train. data). Loss: 0.009975163266062737\n",
      "Training log: 13 epoch (5248 / 60000 train. data). Loss: 0.0252190250903368\n",
      "Training log: 13 epoch (6528 / 60000 train. data). Loss: 0.01824173331260681\n",
      "Training log: 13 epoch (7808 / 60000 train. data). Loss: 0.012964798137545586\n",
      "Training log: 13 epoch (9088 / 60000 train. data). Loss: 0.022151028737425804\n",
      "Training log: 13 epoch (10368 / 60000 train. data). Loss: 0.027608763426542282\n",
      "Training log: 13 epoch (11648 / 60000 train. data). Loss: 0.00659581832587719\n",
      "Training log: 13 epoch (12928 / 60000 train. data). Loss: 0.01961689256131649\n",
      "Training log: 13 epoch (14208 / 60000 train. data). Loss: 0.007544384803622961\n",
      "Training log: 13 epoch (15488 / 60000 train. data). Loss: 0.027754303067922592\n",
      "Training log: 13 epoch (16768 / 60000 train. data). Loss: 0.008786619640886784\n",
      "Training log: 13 epoch (18048 / 60000 train. data). Loss: 0.02126736380159855\n",
      "Training log: 13 epoch (19328 / 60000 train. data). Loss: 0.004363598767668009\n",
      "Training log: 13 epoch (20608 / 60000 train. data). Loss: 0.06118793040513992\n",
      "Training log: 13 epoch (21888 / 60000 train. data). Loss: 0.017303653061389923\n",
      "Training log: 13 epoch (23168 / 60000 train. data). Loss: 0.034611254930496216\n",
      "Training log: 13 epoch (24448 / 60000 train. data). Loss: 0.013989347964525223\n",
      "Training log: 13 epoch (25728 / 60000 train. data). Loss: 0.012018745765089989\n",
      "Training log: 13 epoch (27008 / 60000 train. data). Loss: 0.03150572627782822\n",
      "Training log: 13 epoch (28288 / 60000 train. data). Loss: 0.014611217193305492\n",
      "Training log: 13 epoch (29568 / 60000 train. data). Loss: 0.02850579097867012\n",
      "Training log: 13 epoch (30848 / 60000 train. data). Loss: 0.05583196505904198\n",
      "Training log: 13 epoch (32128 / 60000 train. data). Loss: 0.015688849613070488\n",
      "Training log: 13 epoch (33408 / 60000 train. data). Loss: 0.009934128262102604\n",
      "Training log: 13 epoch (34688 / 60000 train. data). Loss: 0.014034190215170383\n",
      "Training log: 13 epoch (35968 / 60000 train. data). Loss: 0.030181242153048515\n",
      "Training log: 13 epoch (37248 / 60000 train. data). Loss: 0.006408315151929855\n",
      "Training log: 13 epoch (38528 / 60000 train. data). Loss: 0.007790092378854752\n",
      "Training log: 13 epoch (39808 / 60000 train. data). Loss: 0.010573585517704487\n",
      "Training log: 13 epoch (41088 / 60000 train. data). Loss: 0.01834915578365326\n",
      "Training log: 13 epoch (42368 / 60000 train. data). Loss: 0.013456459157168865\n",
      "Training log: 13 epoch (43648 / 60000 train. data). Loss: 0.010260366834700108\n",
      "Training log: 13 epoch (44928 / 60000 train. data). Loss: 0.011512532830238342\n",
      "Training log: 13 epoch (46208 / 60000 train. data). Loss: 0.008116502314805984\n",
      "Training log: 13 epoch (47488 / 60000 train. data). Loss: 0.023475147783756256\n",
      "Training log: 13 epoch (48768 / 60000 train. data). Loss: 0.004455581307411194\n",
      "Training log: 13 epoch (50048 / 60000 train. data). Loss: 0.010775581002235413\n",
      "Training log: 13 epoch (51328 / 60000 train. data). Loss: 0.010723328217864037\n",
      "Training log: 13 epoch (52608 / 60000 train. data). Loss: 0.02795061469078064\n",
      "Training log: 13 epoch (53888 / 60000 train. data). Loss: 0.09857922047376633\n",
      "Training log: 13 epoch (55168 / 60000 train. data). Loss: 0.038694996386766434\n",
      "Training log: 13 epoch (56448 / 60000 train. data). Loss: 0.007690300699323416\n",
      "Training log: 13 epoch (57728 / 60000 train. data). Loss: 0.022915320470929146\n",
      "Training log: 13 epoch (59008 / 60000 train. data). Loss: 0.0529869869351387\n",
      "Test loss (avg): 0.07035993129014968, Accuracy: 0.9788\n",
      "Training log: 14 epoch (128 / 60000 train. data). Loss: 0.019356900826096535\n",
      "Training log: 14 epoch (1408 / 60000 train. data). Loss: 0.012552893720567226\n",
      "Training log: 14 epoch (2688 / 60000 train. data). Loss: 0.005562543869018555\n",
      "Training log: 14 epoch (3968 / 60000 train. data). Loss: 0.025710370391607285\n",
      "Training log: 14 epoch (5248 / 60000 train. data). Loss: 0.021011361852288246\n",
      "Training log: 14 epoch (6528 / 60000 train. data). Loss: 0.03245806694030762\n",
      "Training log: 14 epoch (7808 / 60000 train. data). Loss: 0.024179313331842422\n",
      "Training log: 14 epoch (9088 / 60000 train. data). Loss: 0.015800779685378075\n",
      "Training log: 14 epoch (10368 / 60000 train. data). Loss: 0.015467224642634392\n",
      "Training log: 14 epoch (11648 / 60000 train. data). Loss: 0.06894862651824951\n",
      "Training log: 14 epoch (12928 / 60000 train. data). Loss: 0.02465124800801277\n",
      "Training log: 14 epoch (14208 / 60000 train. data). Loss: 0.008600946515798569\n",
      "Training log: 14 epoch (15488 / 60000 train. data). Loss: 0.015490831807255745\n",
      "Training log: 14 epoch (16768 / 60000 train. data). Loss: 0.0201229527592659\n",
      "Training log: 14 epoch (18048 / 60000 train. data). Loss: 0.011437760666012764\n",
      "Training log: 14 epoch (19328 / 60000 train. data). Loss: 0.008875028230249882\n",
      "Training log: 14 epoch (20608 / 60000 train. data). Loss: 0.02639642357826233\n",
      "Training log: 14 epoch (21888 / 60000 train. data). Loss: 0.035614244639873505\n",
      "Training log: 14 epoch (23168 / 60000 train. data). Loss: 0.005935453809797764\n",
      "Training log: 14 epoch (24448 / 60000 train. data). Loss: 0.013610665686428547\n",
      "Training log: 14 epoch (25728 / 60000 train. data). Loss: 0.00850759819149971\n",
      "Training log: 14 epoch (27008 / 60000 train. data). Loss: 0.007937979884445667\n",
      "Training log: 14 epoch (28288 / 60000 train. data). Loss: 0.01502178143709898\n",
      "Training log: 14 epoch (29568 / 60000 train. data). Loss: 0.012261616066098213\n",
      "Training log: 14 epoch (30848 / 60000 train. data). Loss: 0.03155243769288063\n",
      "Training log: 14 epoch (32128 / 60000 train. data). Loss: 0.010441876016557217\n",
      "Training log: 14 epoch (33408 / 60000 train. data). Loss: 0.0030164343770593405\n",
      "Training log: 14 epoch (34688 / 60000 train. data). Loss: 0.011375627480447292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 14 epoch (35968 / 60000 train. data). Loss: 0.031278952956199646\n",
      "Training log: 14 epoch (37248 / 60000 train. data). Loss: 0.015151750296354294\n",
      "Training log: 14 epoch (38528 / 60000 train. data). Loss: 0.012152510695159435\n",
      "Training log: 14 epoch (39808 / 60000 train. data). Loss: 0.03507557883858681\n",
      "Training log: 14 epoch (41088 / 60000 train. data). Loss: 0.016289224848151207\n",
      "Training log: 14 epoch (42368 / 60000 train. data). Loss: 0.012484850361943245\n",
      "Training log: 14 epoch (43648 / 60000 train. data). Loss: 0.014220657758414745\n",
      "Training log: 14 epoch (44928 / 60000 train. data). Loss: 0.025290513411164284\n",
      "Training log: 14 epoch (46208 / 60000 train. data). Loss: 0.009427745826542377\n",
      "Training log: 14 epoch (47488 / 60000 train. data). Loss: 0.014678113162517548\n",
      "Training log: 14 epoch (48768 / 60000 train. data). Loss: 0.01576312445104122\n",
      "Training log: 14 epoch (50048 / 60000 train. data). Loss: 0.020564109086990356\n",
      "Training log: 14 epoch (51328 / 60000 train. data). Loss: 0.04091772809624672\n",
      "Training log: 14 epoch (52608 / 60000 train. data). Loss: 0.025347229093313217\n",
      "Training log: 14 epoch (53888 / 60000 train. data). Loss: 0.030402425676584244\n",
      "Training log: 14 epoch (55168 / 60000 train. data). Loss: 0.06675764918327332\n",
      "Training log: 14 epoch (56448 / 60000 train. data). Loss: 0.01657053269445896\n",
      "Training log: 14 epoch (57728 / 60000 train. data). Loss: 0.01863275282084942\n",
      "Training log: 14 epoch (59008 / 60000 train. data). Loss: 0.014718826860189438\n",
      "Test loss (avg): 0.06674933256655931, Accuracy: 0.9798\n",
      "Training log: 15 epoch (128 / 60000 train. data). Loss: 0.02195778489112854\n",
      "Training log: 15 epoch (1408 / 60000 train. data). Loss: 0.016025979071855545\n",
      "Training log: 15 epoch (2688 / 60000 train. data). Loss: 0.011522137559950352\n",
      "Training log: 15 epoch (3968 / 60000 train. data). Loss: 0.01545577123761177\n",
      "Training log: 15 epoch (5248 / 60000 train. data). Loss: 0.004793982952833176\n",
      "Training log: 15 epoch (6528 / 60000 train. data). Loss: 0.01557591836899519\n",
      "Training log: 15 epoch (7808 / 60000 train. data). Loss: 0.0036923845764249563\n",
      "Training log: 15 epoch (9088 / 60000 train. data). Loss: 0.008504399098455906\n",
      "Training log: 15 epoch (10368 / 60000 train. data). Loss: 0.006014054175466299\n",
      "Training log: 15 epoch (11648 / 60000 train. data). Loss: 0.015965024009346962\n",
      "Training log: 15 epoch (12928 / 60000 train. data). Loss: 0.011790201999247074\n",
      "Training log: 15 epoch (14208 / 60000 train. data). Loss: 0.014672218821942806\n",
      "Training log: 15 epoch (15488 / 60000 train. data). Loss: 0.01741369068622589\n",
      "Training log: 15 epoch (16768 / 60000 train. data). Loss: 0.014706674963235855\n",
      "Training log: 15 epoch (18048 / 60000 train. data). Loss: 0.004777088295668364\n",
      "Training log: 15 epoch (19328 / 60000 train. data). Loss: 0.009977233596146107\n",
      "Training log: 15 epoch (20608 / 60000 train. data). Loss: 0.004477715585380793\n",
      "Training log: 15 epoch (21888 / 60000 train. data). Loss: 0.013960152864456177\n",
      "Training log: 15 epoch (23168 / 60000 train. data). Loss: 0.0064361183904111385\n",
      "Training log: 15 epoch (24448 / 60000 train. data). Loss: 0.00427592359483242\n",
      "Training log: 15 epoch (25728 / 60000 train. data). Loss: 0.015637660399079323\n",
      "Training log: 15 epoch (27008 / 60000 train. data). Loss: 0.007882463745772839\n",
      "Training log: 15 epoch (28288 / 60000 train. data). Loss: 0.01441286038607359\n",
      "Training log: 15 epoch (29568 / 60000 train. data). Loss: 0.007839828729629517\n",
      "Training log: 15 epoch (30848 / 60000 train. data). Loss: 0.010070005431771278\n",
      "Training log: 15 epoch (32128 / 60000 train. data). Loss: 0.017665566876530647\n",
      "Training log: 15 epoch (33408 / 60000 train. data). Loss: 0.018528616055846214\n",
      "Training log: 15 epoch (34688 / 60000 train. data). Loss: 0.01784219592809677\n",
      "Training log: 15 epoch (35968 / 60000 train. data). Loss: 0.013983777724206448\n",
      "Training log: 15 epoch (37248 / 60000 train. data). Loss: 0.013512355275452137\n",
      "Training log: 15 epoch (38528 / 60000 train. data). Loss: 0.017108531668782234\n",
      "Training log: 15 epoch (39808 / 60000 train. data). Loss: 0.003284656908363104\n",
      "Training log: 15 epoch (41088 / 60000 train. data). Loss: 0.010604199022054672\n",
      "Training log: 15 epoch (42368 / 60000 train. data). Loss: 0.007509719580411911\n",
      "Training log: 15 epoch (43648 / 60000 train. data). Loss: 0.008885066024959087\n",
      "Training log: 15 epoch (44928 / 60000 train. data). Loss: 0.009836394339799881\n",
      "Training log: 15 epoch (46208 / 60000 train. data). Loss: 0.023769043385982513\n",
      "Training log: 15 epoch (47488 / 60000 train. data). Loss: 0.01466788537800312\n",
      "Training log: 15 epoch (48768 / 60000 train. data). Loss: 0.0077186692506074905\n",
      "Training log: 15 epoch (50048 / 60000 train. data). Loss: 0.008754818700253963\n",
      "Training log: 15 epoch (51328 / 60000 train. data). Loss: 0.025676213204860687\n",
      "Training log: 15 epoch (52608 / 60000 train. data). Loss: 0.01543318573385477\n",
      "Training log: 15 epoch (53888 / 60000 train. data). Loss: 0.007784035988152027\n",
      "Training log: 15 epoch (55168 / 60000 train. data). Loss: 0.05451398715376854\n",
      "Training log: 15 epoch (56448 / 60000 train. data). Loss: 0.026657380163669586\n",
      "Training log: 15 epoch (57728 / 60000 train. data). Loss: 0.01176335010677576\n",
      "Training log: 15 epoch (59008 / 60000 train. data). Loss: 0.015126570127904415\n",
      "Test loss (avg): 0.06444985201954842, Accuracy: 0.9811\n",
      "Training log: 16 epoch (128 / 60000 train. data). Loss: 0.04863986372947693\n",
      "Training log: 16 epoch (1408 / 60000 train. data). Loss: 0.005163955967873335\n",
      "Training log: 16 epoch (2688 / 60000 train. data). Loss: 0.04315870255231857\n",
      "Training log: 16 epoch (3968 / 60000 train. data). Loss: 0.01851842552423477\n",
      "Training log: 16 epoch (5248 / 60000 train. data). Loss: 0.004901354666799307\n",
      "Training log: 16 epoch (6528 / 60000 train. data). Loss: 0.005548630841076374\n",
      "Training log: 16 epoch (7808 / 60000 train. data). Loss: 0.0104054631665349\n",
      "Training log: 16 epoch (9088 / 60000 train. data). Loss: 0.006594050209969282\n",
      "Training log: 16 epoch (10368 / 60000 train. data). Loss: 0.020528387278318405\n",
      "Training log: 16 epoch (11648 / 60000 train. data). Loss: 0.008554075844585896\n",
      "Training log: 16 epoch (12928 / 60000 train. data). Loss: 0.02116723731160164\n",
      "Training log: 16 epoch (14208 / 60000 train. data). Loss: 0.04016388952732086\n",
      "Training log: 16 epoch (15488 / 60000 train. data). Loss: 0.013830889016389847\n",
      "Training log: 16 epoch (16768 / 60000 train. data). Loss: 0.004738497547805309\n",
      "Training log: 16 epoch (18048 / 60000 train. data). Loss: 0.008807025849819183\n",
      "Training log: 16 epoch (19328 / 60000 train. data). Loss: 0.005370807833969593\n",
      "Training log: 16 epoch (20608 / 60000 train. data). Loss: 0.021615713834762573\n",
      "Training log: 16 epoch (21888 / 60000 train. data). Loss: 0.02211764082312584\n",
      "Training log: 16 epoch (23168 / 60000 train. data). Loss: 0.006271195597946644\n",
      "Training log: 16 epoch (24448 / 60000 train. data). Loss: 0.01803261786699295\n",
      "Training log: 16 epoch (25728 / 60000 train. data). Loss: 0.009424236603081226\n",
      "Training log: 16 epoch (27008 / 60000 train. data). Loss: 0.007520097307860851\n",
      "Training log: 16 epoch (28288 / 60000 train. data). Loss: 0.009847012348473072\n",
      "Training log: 16 epoch (29568 / 60000 train. data). Loss: 0.0047775451093912125\n",
      "Training log: 16 epoch (30848 / 60000 train. data). Loss: 0.004740492906421423\n",
      "Training log: 16 epoch (32128 / 60000 train. data). Loss: 0.003915624227374792\n",
      "Training log: 16 epoch (33408 / 60000 train. data). Loss: 0.028586488217115402\n",
      "Training log: 16 epoch (34688 / 60000 train. data). Loss: 0.01586369052529335\n",
      "Training log: 16 epoch (35968 / 60000 train. data). Loss: 0.01927793212234974\n",
      "Training log: 16 epoch (37248 / 60000 train. data). Loss: 0.011961991898715496\n",
      "Training log: 16 epoch (38528 / 60000 train. data). Loss: 0.017410235479474068\n",
      "Training log: 16 epoch (39808 / 60000 train. data). Loss: 0.007353406399488449\n",
      "Training log: 16 epoch (41088 / 60000 train. data). Loss: 0.006619700230658054\n",
      "Training log: 16 epoch (42368 / 60000 train. data). Loss: 0.010605566203594208\n",
      "Training log: 16 epoch (43648 / 60000 train. data). Loss: 0.0037102277856320143\n",
      "Training log: 16 epoch (44928 / 60000 train. data). Loss: 0.017686378210783005\n",
      "Training log: 16 epoch (46208 / 60000 train. data). Loss: 0.015650609508156776\n",
      "Training log: 16 epoch (47488 / 60000 train. data). Loss: 0.02632666938006878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 16 epoch (48768 / 60000 train. data). Loss: 0.008262150920927525\n",
      "Training log: 16 epoch (50048 / 60000 train. data). Loss: 0.01112847775220871\n",
      "Training log: 16 epoch (51328 / 60000 train. data). Loss: 0.011053076013922691\n",
      "Training log: 16 epoch (52608 / 60000 train. data). Loss: 0.02626205049455166\n",
      "Training log: 16 epoch (53888 / 60000 train. data). Loss: 0.010701760649681091\n",
      "Training log: 16 epoch (55168 / 60000 train. data). Loss: 0.0031612035818398\n",
      "Training log: 16 epoch (56448 / 60000 train. data). Loss: 0.011190321296453476\n",
      "Training log: 16 epoch (57728 / 60000 train. data). Loss: 0.00875909999012947\n",
      "Training log: 16 epoch (59008 / 60000 train. data). Loss: 0.01616249792277813\n",
      "Test loss (avg): 0.0625580221167067, Accuracy: 0.981\n",
      "Training log: 17 epoch (128 / 60000 train. data). Loss: 0.02952992171049118\n",
      "Training log: 17 epoch (1408 / 60000 train. data). Loss: 0.0023753060959279537\n",
      "Training log: 17 epoch (2688 / 60000 train. data). Loss: 0.01431434415280819\n",
      "Training log: 17 epoch (3968 / 60000 train. data). Loss: 0.052627161145210266\n",
      "Training log: 17 epoch (5248 / 60000 train. data). Loss: 0.01157593633979559\n",
      "Training log: 17 epoch (6528 / 60000 train. data). Loss: 0.0032361431512981653\n",
      "Training log: 17 epoch (7808 / 60000 train. data). Loss: 0.005074266344308853\n",
      "Training log: 17 epoch (9088 / 60000 train. data). Loss: 0.00512384157627821\n",
      "Training log: 17 epoch (10368 / 60000 train. data). Loss: 0.005994448903948069\n",
      "Training log: 17 epoch (11648 / 60000 train. data). Loss: 0.003388781799003482\n",
      "Training log: 17 epoch (12928 / 60000 train. data). Loss: 0.003946845885366201\n",
      "Training log: 17 epoch (14208 / 60000 train. data). Loss: 0.008788441307842731\n",
      "Training log: 17 epoch (15488 / 60000 train. data). Loss: 0.0046956464648246765\n",
      "Training log: 17 epoch (16768 / 60000 train. data). Loss: 0.013290021568536758\n",
      "Training log: 17 epoch (18048 / 60000 train. data). Loss: 0.0072849649004638195\n",
      "Training log: 17 epoch (19328 / 60000 train. data). Loss: 0.005269485525786877\n",
      "Training log: 17 epoch (20608 / 60000 train. data). Loss: 0.019389815628528595\n",
      "Training log: 17 epoch (21888 / 60000 train. data). Loss: 0.008348402567207813\n",
      "Training log: 17 epoch (23168 / 60000 train. data). Loss: 0.00961277261376381\n",
      "Training log: 17 epoch (24448 / 60000 train. data). Loss: 0.009336153976619244\n",
      "Training log: 17 epoch (25728 / 60000 train. data). Loss: 0.02040514536201954\n",
      "Training log: 17 epoch (27008 / 60000 train. data). Loss: 0.006425807252526283\n",
      "Training log: 17 epoch (28288 / 60000 train. data). Loss: 0.008809415623545647\n",
      "Training log: 17 epoch (29568 / 60000 train. data). Loss: 0.009303621016442776\n",
      "Training log: 17 epoch (30848 / 60000 train. data). Loss: 0.0068632857874035835\n",
      "Training log: 17 epoch (32128 / 60000 train. data). Loss: 0.004627740941941738\n",
      "Training log: 17 epoch (33408 / 60000 train. data). Loss: 0.006996002979576588\n",
      "Training log: 17 epoch (34688 / 60000 train. data). Loss: 0.011715752072632313\n",
      "Training log: 17 epoch (35968 / 60000 train. data). Loss: 0.032144200056791306\n",
      "Training log: 17 epoch (37248 / 60000 train. data). Loss: 0.006370233837515116\n",
      "Training log: 17 epoch (38528 / 60000 train. data). Loss: 0.0036082006990909576\n",
      "Training log: 17 epoch (39808 / 60000 train. data). Loss: 0.003302773227915168\n",
      "Training log: 17 epoch (41088 / 60000 train. data). Loss: 0.0025524019729346037\n",
      "Training log: 17 epoch (42368 / 60000 train. data). Loss: 0.008399327285587788\n",
      "Training log: 17 epoch (43648 / 60000 train. data). Loss: 0.018763674423098564\n",
      "Training log: 17 epoch (44928 / 60000 train. data). Loss: 0.004408200271427631\n",
      "Training log: 17 epoch (46208 / 60000 train. data). Loss: 0.013379757292568684\n",
      "Training log: 17 epoch (47488 / 60000 train. data). Loss: 0.012996475212275982\n",
      "Training log: 17 epoch (48768 / 60000 train. data). Loss: 0.006921543274074793\n",
      "Training log: 17 epoch (50048 / 60000 train. data). Loss: 0.008021133951842785\n",
      "Training log: 17 epoch (51328 / 60000 train. data). Loss: 0.006817723624408245\n",
      "Training log: 17 epoch (52608 / 60000 train. data). Loss: 0.01776302233338356\n",
      "Training log: 17 epoch (53888 / 60000 train. data). Loss: 0.0029388738330453634\n",
      "Training log: 17 epoch (55168 / 60000 train. data). Loss: 0.019359426572918892\n",
      "Training log: 17 epoch (56448 / 60000 train. data). Loss: 0.006724806968122721\n",
      "Training log: 17 epoch (57728 / 60000 train. data). Loss: 0.006633631885051727\n",
      "Training log: 17 epoch (59008 / 60000 train. data). Loss: 0.010400940664112568\n",
      "Test loss (avg): 0.07233684879541397, Accuracy: 0.9787\n",
      "Training log: 18 epoch (128 / 60000 train. data). Loss: 0.007908388040959835\n",
      "Training log: 18 epoch (1408 / 60000 train. data). Loss: 0.019230768084526062\n",
      "Training log: 18 epoch (2688 / 60000 train. data). Loss: 0.007569948211312294\n",
      "Training log: 18 epoch (3968 / 60000 train. data). Loss: 0.006287059746682644\n",
      "Training log: 18 epoch (5248 / 60000 train. data). Loss: 0.013142975978553295\n",
      "Training log: 18 epoch (6528 / 60000 train. data). Loss: 0.005715136416256428\n",
      "Training log: 18 epoch (7808 / 60000 train. data). Loss: 0.007064350880682468\n",
      "Training log: 18 epoch (9088 / 60000 train. data). Loss: 0.007511668372899294\n",
      "Training log: 18 epoch (10368 / 60000 train. data). Loss: 0.01819174736738205\n",
      "Training log: 18 epoch (11648 / 60000 train. data). Loss: 0.007768032141029835\n",
      "Training log: 18 epoch (12928 / 60000 train. data). Loss: 0.00726169440895319\n",
      "Training log: 18 epoch (14208 / 60000 train. data). Loss: 0.00376461585983634\n",
      "Training log: 18 epoch (15488 / 60000 train. data). Loss: 0.033473946154117584\n",
      "Training log: 18 epoch (16768 / 60000 train. data). Loss: 0.006508444435894489\n",
      "Training log: 18 epoch (18048 / 60000 train. data). Loss: 0.007791105657815933\n",
      "Training log: 18 epoch (19328 / 60000 train. data). Loss: 0.007604826241731644\n",
      "Training log: 18 epoch (20608 / 60000 train. data). Loss: 0.013574962504208088\n",
      "Training log: 18 epoch (21888 / 60000 train. data). Loss: 0.008569522760808468\n",
      "Training log: 18 epoch (23168 / 60000 train. data). Loss: 0.0027841366827487946\n",
      "Training log: 18 epoch (24448 / 60000 train. data). Loss: 0.0018709506839513779\n",
      "Training log: 18 epoch (25728 / 60000 train. data). Loss: 0.0027049481868743896\n",
      "Training log: 18 epoch (27008 / 60000 train. data). Loss: 0.005783988628536463\n",
      "Training log: 18 epoch (28288 / 60000 train. data). Loss: 0.0006910192314535379\n",
      "Training log: 18 epoch (29568 / 60000 train. data). Loss: 0.0032612564973533154\n",
      "Training log: 18 epoch (30848 / 60000 train. data). Loss: 0.00672867801040411\n",
      "Training log: 18 epoch (32128 / 60000 train. data). Loss: 0.0029904653783887625\n",
      "Training log: 18 epoch (33408 / 60000 train. data). Loss: 0.004058921709656715\n",
      "Training log: 18 epoch (34688 / 60000 train. data). Loss: 0.0045767417177557945\n",
      "Training log: 18 epoch (35968 / 60000 train. data). Loss: 0.006103494670242071\n",
      "Training log: 18 epoch (37248 / 60000 train. data). Loss: 0.013028444722294807\n",
      "Training log: 18 epoch (38528 / 60000 train. data). Loss: 0.003744203131645918\n",
      "Training log: 18 epoch (39808 / 60000 train. data). Loss: 0.006709462031722069\n",
      "Training log: 18 epoch (41088 / 60000 train. data). Loss: 0.008838964626193047\n",
      "Training log: 18 epoch (42368 / 60000 train. data). Loss: 0.005316780414432287\n",
      "Training log: 18 epoch (43648 / 60000 train. data). Loss: 0.01822740025818348\n",
      "Training log: 18 epoch (44928 / 60000 train. data). Loss: 0.014106305316090584\n",
      "Training log: 18 epoch (46208 / 60000 train. data). Loss: 0.005876408889889717\n",
      "Training log: 18 epoch (47488 / 60000 train. data). Loss: 0.00471658306196332\n",
      "Training log: 18 epoch (48768 / 60000 train. data). Loss: 0.0175800621509552\n",
      "Training log: 18 epoch (50048 / 60000 train. data). Loss: 0.004997223615646362\n",
      "Training log: 18 epoch (51328 / 60000 train. data). Loss: 0.0038754367269575596\n",
      "Training log: 18 epoch (52608 / 60000 train. data). Loss: 0.016315460205078125\n",
      "Training log: 18 epoch (53888 / 60000 train. data). Loss: 0.009691746905446053\n",
      "Training log: 18 epoch (55168 / 60000 train. data). Loss: 0.0005752716679126024\n",
      "Training log: 18 epoch (56448 / 60000 train. data). Loss: 0.006293356418609619\n",
      "Training log: 18 epoch (57728 / 60000 train. data). Loss: 0.007261712569743395\n",
      "Training log: 18 epoch (59008 / 60000 train. data). Loss: 0.011834049597382545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss (avg): 0.061536193948984144, Accuracy: 0.9822\n",
      "Training log: 19 epoch (128 / 60000 train. data). Loss: 0.00863226130604744\n",
      "Training log: 19 epoch (1408 / 60000 train. data). Loss: 0.010167337022721767\n",
      "Training log: 19 epoch (2688 / 60000 train. data). Loss: 0.004936558194458485\n",
      "Training log: 19 epoch (3968 / 60000 train. data). Loss: 0.002090857829898596\n",
      "Training log: 19 epoch (5248 / 60000 train. data). Loss: 0.0041291117668151855\n",
      "Training log: 19 epoch (6528 / 60000 train. data). Loss: 0.0017516578081995249\n",
      "Training log: 19 epoch (7808 / 60000 train. data). Loss: 0.010481477715075016\n",
      "Training log: 19 epoch (9088 / 60000 train. data). Loss: 0.006182265002280474\n",
      "Training log: 19 epoch (10368 / 60000 train. data). Loss: 0.006285377312451601\n",
      "Training log: 19 epoch (11648 / 60000 train. data). Loss: 0.003549168584868312\n",
      "Training log: 19 epoch (12928 / 60000 train. data). Loss: 0.0021062083542346954\n",
      "Training log: 19 epoch (14208 / 60000 train. data). Loss: 0.0036505151074379683\n",
      "Training log: 19 epoch (15488 / 60000 train. data). Loss: 0.005409579258412123\n",
      "Training log: 19 epoch (16768 / 60000 train. data). Loss: 0.014974098652601242\n",
      "Training log: 19 epoch (18048 / 60000 train. data). Loss: 0.00931539200246334\n",
      "Training log: 19 epoch (19328 / 60000 train. data). Loss: 0.0019180246163159609\n",
      "Training log: 19 epoch (20608 / 60000 train. data). Loss: 0.005694391205906868\n",
      "Training log: 19 epoch (21888 / 60000 train. data). Loss: 0.004627580754458904\n",
      "Training log: 19 epoch (23168 / 60000 train. data). Loss: 0.004255244974046946\n",
      "Training log: 19 epoch (24448 / 60000 train. data). Loss: 0.004966236185282469\n",
      "Training log: 19 epoch (25728 / 60000 train. data). Loss: 0.004555743187665939\n",
      "Training log: 19 epoch (27008 / 60000 train. data). Loss: 0.0026786266826093197\n",
      "Training log: 19 epoch (28288 / 60000 train. data). Loss: 0.002870911732316017\n",
      "Training log: 19 epoch (29568 / 60000 train. data). Loss: 0.005286006256937981\n",
      "Training log: 19 epoch (30848 / 60000 train. data). Loss: 0.006688664201647043\n",
      "Training log: 19 epoch (32128 / 60000 train. data). Loss: 0.011205759830772877\n",
      "Training log: 19 epoch (33408 / 60000 train. data). Loss: 0.00462469132617116\n",
      "Training log: 19 epoch (34688 / 60000 train. data). Loss: 0.0063145398162305355\n",
      "Training log: 19 epoch (35968 / 60000 train. data). Loss: 0.0022561566438525915\n",
      "Training log: 19 epoch (37248 / 60000 train. data). Loss: 0.004523905459791422\n",
      "Training log: 19 epoch (38528 / 60000 train. data). Loss: 0.0015359873650595546\n",
      "Training log: 19 epoch (39808 / 60000 train. data). Loss: 0.00447880057618022\n",
      "Training log: 19 epoch (41088 / 60000 train. data). Loss: 0.04439086467027664\n",
      "Training log: 19 epoch (42368 / 60000 train. data). Loss: 0.009755328297615051\n",
      "Training log: 19 epoch (43648 / 60000 train. data). Loss: 0.0036403669510036707\n",
      "Training log: 19 epoch (44928 / 60000 train. data). Loss: 0.013363522477447987\n",
      "Training log: 19 epoch (46208 / 60000 train. data). Loss: 0.004541425500065088\n",
      "Training log: 19 epoch (47488 / 60000 train. data). Loss: 0.002477153902873397\n",
      "Training log: 19 epoch (48768 / 60000 train. data). Loss: 0.013026246801018715\n",
      "Training log: 19 epoch (50048 / 60000 train. data). Loss: 0.002592137549072504\n",
      "Training log: 19 epoch (51328 / 60000 train. data). Loss: 0.0037177754566073418\n",
      "Training log: 19 epoch (52608 / 60000 train. data). Loss: 0.0035907570272684097\n",
      "Training log: 19 epoch (53888 / 60000 train. data). Loss: 0.006207064259797335\n",
      "Training log: 19 epoch (55168 / 60000 train. data). Loss: 0.004807927180081606\n",
      "Training log: 19 epoch (56448 / 60000 train. data). Loss: 0.005986891686916351\n",
      "Training log: 19 epoch (57728 / 60000 train. data). Loss: 0.024876849725842476\n",
      "Training log: 19 epoch (59008 / 60000 train. data). Loss: 0.00774141401052475\n",
      "Test loss (avg): 0.06445375111408648, Accuracy: 0.9818\n",
      "Training log: 20 epoch (128 / 60000 train. data). Loss: 0.0019043375505134463\n",
      "Training log: 20 epoch (1408 / 60000 train. data). Loss: 0.006671415641903877\n",
      "Training log: 20 epoch (2688 / 60000 train. data). Loss: 0.002663983264937997\n",
      "Training log: 20 epoch (3968 / 60000 train. data). Loss: 0.0017334215808659792\n",
      "Training log: 20 epoch (5248 / 60000 train. data). Loss: 0.006947419606149197\n",
      "Training log: 20 epoch (6528 / 60000 train. data). Loss: 0.010160846635699272\n",
      "Training log: 20 epoch (7808 / 60000 train. data). Loss: 0.003033147193491459\n",
      "Training log: 20 epoch (9088 / 60000 train. data). Loss: 0.0025852711405605078\n",
      "Training log: 20 epoch (10368 / 60000 train. data). Loss: 0.0016095030587166548\n",
      "Training log: 20 epoch (11648 / 60000 train. data). Loss: 0.0028395624831318855\n",
      "Training log: 20 epoch (12928 / 60000 train. data). Loss: 0.0010165050625801086\n",
      "Training log: 20 epoch (14208 / 60000 train. data). Loss: 0.0056344810873270035\n",
      "Training log: 20 epoch (15488 / 60000 train. data). Loss: 0.0013062474317848682\n",
      "Training log: 20 epoch (16768 / 60000 train. data). Loss: 0.0040732561610639095\n",
      "Training log: 20 epoch (18048 / 60000 train. data). Loss: 0.003178660525009036\n",
      "Training log: 20 epoch (19328 / 60000 train. data). Loss: 0.0032433702144771814\n",
      "Training log: 20 epoch (20608 / 60000 train. data). Loss: 0.01346145011484623\n",
      "Training log: 20 epoch (21888 / 60000 train. data). Loss: 0.00915351789444685\n",
      "Training log: 20 epoch (23168 / 60000 train. data). Loss: 0.0030909182969480753\n",
      "Training log: 20 epoch (24448 / 60000 train. data). Loss: 0.00637572119012475\n",
      "Training log: 20 epoch (25728 / 60000 train. data). Loss: 0.003635948058217764\n",
      "Training log: 20 epoch (27008 / 60000 train. data). Loss: 0.0023173189256340265\n",
      "Training log: 20 epoch (28288 / 60000 train. data). Loss: 0.004795518238097429\n",
      "Training log: 20 epoch (29568 / 60000 train. data). Loss: 0.0065309046767652035\n",
      "Training log: 20 epoch (30848 / 60000 train. data). Loss: 0.004072184674441814\n",
      "Training log: 20 epoch (32128 / 60000 train. data). Loss: 0.002587592927739024\n",
      "Training log: 20 epoch (33408 / 60000 train. data). Loss: 0.003289060201495886\n",
      "Training log: 20 epoch (34688 / 60000 train. data). Loss: 0.0028269316535443068\n",
      "Training log: 20 epoch (35968 / 60000 train. data). Loss: 0.0020316727459430695\n",
      "Training log: 20 epoch (37248 / 60000 train. data). Loss: 0.0022017566952854395\n",
      "Training log: 20 epoch (38528 / 60000 train. data). Loss: 0.006719836033880711\n",
      "Training log: 20 epoch (39808 / 60000 train. data). Loss: 0.003215763485059142\n",
      "Training log: 20 epoch (41088 / 60000 train. data). Loss: 0.006778663024306297\n",
      "Training log: 20 epoch (42368 / 60000 train. data). Loss: 0.0030057826079428196\n",
      "Training log: 20 epoch (43648 / 60000 train. data). Loss: 0.019729087129235268\n",
      "Training log: 20 epoch (44928 / 60000 train. data). Loss: 0.009622967801988125\n",
      "Training log: 20 epoch (46208 / 60000 train. data). Loss: 0.008826558478176594\n",
      "Training log: 20 epoch (47488 / 60000 train. data). Loss: 0.006999794393777847\n",
      "Training log: 20 epoch (48768 / 60000 train. data). Loss: 0.004775804001837969\n",
      "Training log: 20 epoch (50048 / 60000 train. data). Loss: 0.007881088182330132\n",
      "Training log: 20 epoch (51328 / 60000 train. data). Loss: 0.0033360025845468044\n",
      "Training log: 20 epoch (52608 / 60000 train. data). Loss: 0.0029668991919606924\n",
      "Training log: 20 epoch (53888 / 60000 train. data). Loss: 0.003032090375199914\n",
      "Training log: 20 epoch (55168 / 60000 train. data). Loss: 0.028482386842370033\n",
      "Training log: 20 epoch (56448 / 60000 train. data). Loss: 0.007794660050421953\n",
      "Training log: 20 epoch (57728 / 60000 train. data). Loss: 0.003584685968235135\n",
      "Training log: 20 epoch (59008 / 60000 train. data). Loss: 0.010684682056307793\n",
      "Test loss (avg): 0.07233433063030242, Accuracy: 0.9795\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 学習回数\n",
    "epoch = 20\n",
    "\n",
    "# 学習結果の保存用\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "}\n",
    "\n",
    "# ネットワークを構築\n",
    "net: torch.nn.Module = Net()\n",
    "\n",
    "# MNISTのデータローダーを取得\n",
    "loaders = load_MNIST()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
    "\n",
    "for e in range(epoch):\n",
    "\n",
    "    \"\"\" Training Part\"\"\"\n",
    "    loss = None\n",
    "\n",
    "    # 学習開始 (再開)\n",
    "    net.train(True)  # 引数は省略可能\n",
    "\n",
    "    for i, (data, target) in enumerate(loaders['train']):\n",
    "        data = data.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = f.nll_loss(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Training log: {} epoch ({} / 60000 train. data). Loss: {}'.format(e+1,\n",
    "                                                                                     (i+1)*128,\n",
    "                                                                                     loss.item())\n",
    "                     )\n",
    "\n",
    "    history['train_loss'].append(loss)\n",
    "\n",
    "    \"\"\" Test Part \"\"\"\n",
    "    # 学習のストップ\n",
    "    net.eval()  # または net.train(False) でも良い\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data = data.view(-1, 28 * 28)\n",
    "            output = net(data)\n",
    "            test_loss += f.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= 10000\n",
    "\n",
    "    print('Test loss (avg): {}, Accuracy: {}'.format(test_loss,\n",
    "                                                     correct / 10000))\n",
    "\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(correct / 10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93d93d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 結果の出力と描画\n",
    "plt.figure()\n",
    "#plt.plot(range(1, epoch+1), history['train_loss'], label='train_loss')\n",
    "plt.plot(range(1, epoch+1), history['test_loss'], label='test_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('loss.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, epoch+1), history['test_acc'])\n",
    "plt.title('test accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('test_acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd1f3e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9380"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history['train_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee4d0f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3317cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
